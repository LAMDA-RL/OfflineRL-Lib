# OfflineRL-Lib
> ðŸš§ This repo is not ready for release, benchmarking is ongoing. ðŸš§

OfflineRL-Lib provides unofficial and benchmarked PyTorch implementations for selected OfflineRL algorithms, including: 
- [Implicit Q-Learning](https://arxiv.org/abs/2110.06169)
- [Extreme Q-Learning](https://arxiv.org/abs/2301.02328)

Currently OfflineRL-Lib is heavily based off [UtilsRL](https://github.com/typoverflow/UtilsRL), and we will release a standalone version once it is ready. 


## Benchmark Results
When certain design choices, e.g. the choice of autodiff backend (jax or tf or pytorch) vary, the preference for each hyper-parameters may vary as well. Hence when benchmarking, we tested each algorithm's performace in three ways: 
+ **Paper Performance**: the performance reported in white paper;
+ **OfflineRL-Lib (with paper args)**: the performance obtained by using OfflineRL-Lib implementation and the configs in paper or original implementations;
+ **OfflineRL-Lib (with CORL args)**: the performance obtained by using OfflineRL-Lib implementation and the configs in [CORL](https://github.com/tinkoff-ai/CORL). 

> For the last option, arguments are directly borrowed from CORL. CORL provides simplified single-file implementations of these algorithms as well as their finetuned hyper-parameters based on pytorch, please check [their repo](https://github.com/tinkoff-ai/COR) as well. 

### IQL [:page_facing_up:](https://arxiv.org/abs/2110.06169) [:chart_with_upwards_trend:](https://wandb.ai/lamda-rl/IQL-Offline)

<table>
    <thead>
        <tr>
            <th>Task</th>
            <th>Dataset Quality</th>
            <th>Paper Performance</th>
            <th>OfflineRL-Lib<br>(with paper args)</th>
            <th>OfflineRL-Lib<br>(with CORL args)
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan=4>halfcheetah</td>
            <td>random-v2</td><td>NA</td><td>9.4Â±3.9</td><td>13.5Â±3.9</td>
        </tr>
        <tr>
            <td>medium-v2</td><td>47.4</td><td>47.3Â±0.2</td><td>48.6Â±0.2</td>
        </tr>
        <tr>
            <td>medium-replay-v2</td><td>44.2</td><td>43.7Â±0.7</td><td>44.3Â±0.4</td>
        </tr>
        <tr>
            <td>medium-expert-v2</td><td>86.7</td><td>89.7Â±2.9</td><td>93.9Â±1.6</td>
        </tr>
        <tr>
            <td rowspan=4>hopper</td>
            <td>random-v2</td><td>NA</td><td>7.9Â±0.3</td><td>7.3Â±0.1</td>
        </tr>
        <tr>
            <td>medium-v2</td><td>66.3</td><td>64.8Â±7.2</td><td>54.5Â±1.6</td>
        </tr>
        <tr>
            <td>medium-replay-v2</td><td>94.7</td><td>93.4Â±7.9</td><td>41.5Â±23.0</td>
        </tr>
        <tr>
            <td>medium-expert-v2</td><td>91.5</td><td>97.8Â±9.0</td><td>108.1Â±3.1</td>
        </tr>
        <tr>
            <td rowspan=4>walker2d</td>
            <td>random-v2</td><td>NA</td><td>6.0Â±1.0</td><td>3.1Â±0.9</td>
        </tr>
        <tr>
            <td>medium-v2</td><td>78.3</td><td>83.5Â±2.2</td><td>81.3Â±8.7</td>
        </tr>
        <tr>
            <td>medium-replay-v2</td><td>73.9</td><td>66.6Â±16.2</td><td>77.0Â±7.3</td>
        </tr>
        <tr>
            <td>medium-expert-v2</td><td>109.6</td><td>108.9Â±2.5</td><td>112.4Â±0.8</td>
        </tr>
    </tbody>
</table>